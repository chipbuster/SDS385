\documentclass{article}

%% This template is pieces of templates provided by Chi-Kit Lam for
%% CS395T in S2016, bits taken from
%% https://github.com/nykh/latex-shorthands/blob/master/myshorthands.tex, and
%% other configs scrapped together over time.

\PassOptionsToPackage{svgnames}{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{float,graphicx,tabularx}
\usepackage[labelformat=simple]{subcaption}
\usepackage{eqparbox}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{bbm}
\usepackage{leftidx}
\usepackage{mathtools}
\usepackage{semantic}
\usepackage[version=4]{mhchem} % chemical formulae
\usepackage{hyperref}
\usepackage{parskip}  %Extra space on new paragraph, no indent
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{listings}

\renewcommand\thesubfigure{(\alph{subfigure})}

%%% Section Separators

\newcommand{\starhsep}{
\begin{center}
  $\ast$~$\ast$~$\ast$
\end{center}}

\newcommand{\clubhsep}{
\begin{center}
  $\clubsuit$~$\clubsuit$~$\clubsuit$
\end{center}}

%% Paired delimiters, use with \delim{*expression*}
\DeclarePairedDelimiter\abkt{\langle}{\rangle}
\DeclarePairedDelimiter\cbkt{\lbrace}{\rbrace}
\DeclarePairedDelimiter\rbkt{\lparen}{\rparen}
\DeclarePairedDelimiter\sbkt{\lbrack}{\rbrack}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\ceiling{\lceil}{\rceil}
\DeclarePairedDelimiterX\set[2]{\lbrack}{\rbrack}{#1 \colon #2}

%% Operators

\newcommand{\clapover}[2]{\overbrace{#1}_{\mathclap{#2}}}
\newcommand{\clapunder}[2]{\underbrace{#1}_{\mathclap{#2}}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\supp}{supp}
\newcommand{\defeq}{\stackrel{\text{def}}=}

\DeclareMathOperator{\matrixtranspose}{T}
\newcommand{\T}{{\matrixtranspose}}
\DeclareMathOperator{\frobeniusnorm}{F}
\newcommand{\Fro}{{\frobeniusnorm}}
\DeclareMathOperator{\diag}{diag}

\DeclareMathOperator*{\E}{E}
\DeclareMathOperator*{\Var}{Var}
\DeclareMathOperator*{\Cov}{Cov}
\DeclareMathOperator*{\Corr}{Corr}
\DeclareMathOperator{\BinDist}{Bin}
\DeclareMathOperator{\GaussDist}{N}
\DeclareMathOperator{\PoisDist}{Pois}
\DeclareMathOperator{\UnifDist}{U}
\DeclareMathOperator{\erf}{erf} % error function.

\DeclareMathOperator{\df}{d} % Differential form.
\DeclareMathOperator{\round}{\partial}

\DeclareMathOperator{\area}{area}
\DeclareMathOperator{\vol}{vol}

%% Variables. This may seem excessive but trust me it's quite nice

\newcommand{\eps}{\varepsilon}
\newcommand{\haat}{\widehat}

\newcommand{\bbA}{{\mathbb{A}}}
\newcommand{\bbB}{{\mathbb{B}}}
\newcommand{\bbC}{{\mathbb{C}}}
\newcommand{\bbD}{{\mathbb{D}}}
\newcommand{\bbE}{{\mathbb{E}}}
\newcommand{\bbF}{{\mathbb{F}}}
\newcommand{\bbG}{{\mathbb{G}}}
\newcommand{\bbH}{{\mathbb{H}}}
\newcommand{\bbI}{{\mathbb{I}}}
\newcommand{\bbJ}{{\mathbb{J}}}
\newcommand{\bbK}{{\mathbb{K}}}
\newcommand{\bbL}{{\mathbb{L}}}
\newcommand{\bbM}{{\mathbb{M}}}
\newcommand{\bbN}{{\mathbb{N}}}
\newcommand{\bbO}{{\mathbb{O}}}
\newcommand{\bbP}{{\mathbb{P}}}
\newcommand{\bbQ}{{\mathbb{Q}}}
\newcommand{\bbR}{{\mathbb{R}}}
\newcommand{\bbS}{{\mathbb{S}}}
\newcommand{\bbT}{{\mathbb{T}}}
\newcommand{\bbU}{{\mathbb{U}}}
\newcommand{\bbV}{{\mathbb{V}}}
\newcommand{\bbW}{{\mathbb{W}}}
\newcommand{\bbX}{{\mathbb{X}}}
\newcommand{\bbY}{{\mathbb{Y}}}
\newcommand{\bbZ}{{\mathbb{Z}}}
\newcommand{\bbone}{{\mathbbm{1}}}

\newcommand{\bfA}{{\mathbf{A}}}
\newcommand{\bfB}{{\mathbf{B}}}
\newcommand{\bfC}{{\mathbf{C}}}
\newcommand{\bfD}{{\mathbf{D}}}
\newcommand{\bfE}{{\mathbf{E}}}
\newcommand{\bfF}{{\mathbf{F}}}
\newcommand{\bfG}{{\mathbf{G}}}
\newcommand{\bfH}{{\mathbf{H}}}
\newcommand{\bfI}{{\mathbf{I}}}
\newcommand{\bfJ}{{\mathbf{J}}}
\newcommand{\bfK}{{\mathbf{K}}}
\newcommand{\bfL}{{\mathbf{L}}}
\newcommand{\bfM}{{\mathbf{M}}}
\newcommand{\bfN}{{\mathbf{N}}}
\newcommand{\bfO}{{\mathbf{O}}}
\newcommand{\bfP}{{\mathbf{P}}}
\newcommand{\bfQ}{{\mathbf{Q}}}
\newcommand{\bfR}{{\mathbf{R}}}
\newcommand{\bfS}{{\mathbf{S}}}
\newcommand{\bfT}{{\mathbf{T}}}
\newcommand{\bfU}{{\mathbf{U}}}
\newcommand{\bfV}{{\mathbf{V}}}
\newcommand{\bfW}{{\mathbf{W}}}
\newcommand{\bfX}{{\mathbf{X}}}
\newcommand{\bfY}{{\mathbf{Y}}}
\newcommand{\bfZ}{{\mathbf{Z}}}
\newcommand{\bfa}{{\mathbf{a}}}
\newcommand{\bfb}{{\mathbf{b}}}
\newcommand{\bfc}{{\mathbf{c}}}
\newcommand{\bfd}{{\mathbf{d}}}
\newcommand{\bfe}{{\mathbf{e}}}
\newcommand{\bff}{{\mathbf{f}}}
\newcommand{\bfg}{{\mathbf{g}}}
\newcommand{\bfh}{{\mathbf{h}}}
\newcommand{\bfi}{{\mathbf{i}}}
\newcommand{\bfj}{{\mathbf{j}}}
\newcommand{\bfk}{{\mathbf{k}}}
\newcommand{\bfl}{{\mathbf{l}}}
\newcommand{\bfm}{{\mathbf{m}}}
\newcommand{\bfn}{{\mathbf{n}}}
\newcommand{\bfo}{{\mathbf{o}}}
\newcommand{\bfp}{{\mathbf{p}}}
\newcommand{\bfq}{{\mathbf{q}}}
\newcommand{\bfr}{{\mathbf{r}}}
\newcommand{\bfs}{{\mathbf{s}}}
\newcommand{\bft}{{\mathbf{t}}}
\newcommand{\bfu}{{\mathbf{u}}}
\newcommand{\bfv}{{\mathbf{v}}}
\newcommand{\bfw}{{\mathbf{w}}}
\newcommand{\bfx}{{\mathbf{x}}}
\newcommand{\bfy}{{\mathbf{y}}}
\newcommand{\bfz}{{\mathbf{z}}}
\newcommand{\bfzero}{{\mathbf{0}}}
\newcommand{\bfone}{{\mathbf{1}}}
\newcommand{\bfGamma}{{\boldsymbol \Gamma}}
\newcommand{\bfDelta}{{\boldsymbol \Delta}}
\newcommand{\bfTheta}{{\boldsymbol \Theta}}
\newcommand{\bfLambda}{{\boldsymbol \Lambda}}
\newcommand{\bfXi}{{\boldsymbol \Xi}}
\newcommand{\bfPi}{{\boldsymbol \Pi}}
\newcommand{\bfSigma}{{\boldsymbol \Sigma}}
\newcommand{\bfUpsilon}{{\boldsymbol \Upsilon}}
\newcommand{\bfPhi}{{\boldsymbol \Phi}}
\newcommand{\bfPsi}{{\boldsymbol \Psi}}
\newcommand{\bfOmega}{{\boldsymbol \Omega}}
\newcommand{\bfalpha}{{\boldsymbol \alpha}}
\newcommand{\bfbeta}{{\boldsymbol \beta}}
\newcommand{\bfgamma}{{\boldsymbol \gamma}}
\newcommand{\bfdelta}{{\boldsymbol \delta}}
\newcommand{\bfeps}{{\boldsymbol \eps}}
\newcommand{\bfzeta}{{\boldsymbol \zeta}}
\newcommand{\bfeta}{{\boldsymbol \eta}}
\newcommand{\bftheta}{{\boldsymbol \theta}}
\newcommand{\bfiota}{{\boldsymbol \iota}}
\newcommand{\bfkappa}{{\boldsymbol \kappa}}
\newcommand{\bflambda}{{\boldsymbol \lambda}}
\newcommand{\bfmu}{{\boldsymbol \mu}}
\newcommand{\bfnu}{{\boldsymbol \nu}}
\newcommand{\bfxi}{{\boldsymbol \xi}}
\newcommand{\bfpi}{{\boldsymbol \pi}}
\newcommand{\bfrho}{{\boldsymbol \rho}}
\newcommand{\bfsigma}{{\boldsymbol \sigma}}
\newcommand{\bftau}{{\boldsymbol \tau}}
\newcommand{\bfupsilon}{{\boldsymbol \upsilon}}
\newcommand{\bfphi}{{\boldsymbol \phi}}
\newcommand{\bfchi}{{\boldsymbol \chi}}
\newcommand{\bfpsi}{{\boldsymbol \psi}}
\newcommand{\bfomega}{{\boldsymbol \omega}}

\newcommand{\calA}{{\mathcal{A}}}
\newcommand{\calB}{{\mathcal{B}}}
\newcommand{\calC}{{\mathcal{C}}}
\newcommand{\calD}{{\mathcal{D}}}
\newcommand{\calE}{{\mathcal{E}}}
\newcommand{\calF}{{\mathcal{F}}}
\newcommand{\calG}{{\mathcal{G}}}
\newcommand{\calH}{{\mathcal{H}}}
\newcommand{\calI}{{\mathcal{I}}}
\newcommand{\calJ}{{\mathcal{J}}}
\newcommand{\calK}{{\mathcal{K}}}
\newcommand{\calL}{{\mathcal{L}}}
\newcommand{\calM}{{\mathcal{M}}}
\newcommand{\calN}{{\mathcal{N}}}
\newcommand{\calO}{{\mathcal{O}}}
\newcommand{\calP}{{\mathcal{P}}}
\newcommand{\calQ}{{\mathcal{Q}}}
\newcommand{\calR}{{\mathcal{R}}}
\newcommand{\calS}{{\mathcal{S}}}
\newcommand{\calT}{{\mathcal{T}}}
\newcommand{\calU}{{\mathcal{U}}}
\newcommand{\calV}{{\mathcal{V}}}
\newcommand{\calW}{{\mathcal{W}}}
\newcommand{\calX}{{\mathcal{X}}}
\newcommand{\calY}{{\mathcal{Y}}}
\newcommand{\calZ}{{\mathcal{Z}}}

%% Special Group Aliases

\DeclareMathOperator{\groupGF}{GF}
\DeclareMathOperator{\groupGL}{GL}
\DeclareMathOperator{\groupSL}{SL}
\DeclareMathOperator{\groupO}{O}
\DeclareMathOperator{\groupSO}{SO}
\DeclareMathOperator{\groupU}{U}
\DeclareMathOperator{\groupSU}{SU}
\DeclareMathOperator{\groupE}{E}
\DeclareMathOperator{\groupSE}{SE}

\mathlig{---}{\text{---}}

%% Some sane listings defaults

\usepackage{color}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\footnotesize,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,                    % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=Octave,                 % the language of the code
  otherkeywords={*,...},           % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,                     % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}


%%% style.sty ends%%%

\title{SDS 385 Exercise Set 01}
\author{Kevin Song}
\date{\today}

\begin{document}

\maketitle

\section{WLS Objective Function}

The weighted least squares problem is described by the equation

\[
\hat{\beta} = \arg \min_{\beta \in \mathcal{R}^P} \sum_{i=1}^N \frac{w_i}{2}(y_i
- x_i^T \beta)^2 \, ,
\]

where $w_i$ is the weight of the $i$-th observation, $y_i$ is the $i$-th
response, and $x_i$ is the $i$-th row of $X$, an $N \times P$ matrix.

We can rewrite the objective function in a matrix-vector form in a few steps. First, we note that
this is a dot product between the vectors $w$ and $y - X\beta$:

\[
 \sum_{i=1}^N \frac{w_i}{2}(y_i - x_i^T \beta)^2 = w^T (y - X\beta)^2
\]

Unfortunately, this isn't quite valid, since the exponent denotes an
element-wise square, which is something that cannot be done with standard vector
operations.

Instead, we convert to quadratic form:

\[
\frac{1}{2} w^T (y - X\beta)^2 =\frac{1}{2} (y - X\beta)^T W (y - X\beta)
\]

where $W$ is the diagonal matrix with the entries of $w$ along its diagonal.

While I haven't made a formal proof of correctness of this transformation, I
offer the following validity argument: define $p = y - X\beta$. If we say that
the diagonal elements of $W$ are the elements $w$, doing the second multiplication
first yields the vector $q$, where $q_i = w_ip_i$. Then we get that the end
result is $\sum_i p_i w_i p_i$, which is what we wanted.

Expanding this product, we find that

\[
  \frac{1}{2} (y - X\beta)^T W (y - X\beta) =
  \frac{1}{2} \left( y^TWy - y^TWX\beta - \beta^TX^TWy + \beta^TX^TWX\beta \right)
\]

In this expansion, we find that the first term is useless to us: since we change
the value the objective function takes by altering $\beta$, and $\beta$ is not
in this term, it does not affect the optimization. We throw it out.

The second and third terms are transposes of each other. As luck would have it,
they are also scalar terms. Since $a = a^T$ when $a$ is a scalar, we combine the
two terms.

Finally, we recognize that the final term is the quadratic form of $\beta$ with
the matrix $X^TWX$.

This gives us the following objective function:

\[
 f(\beta) = (y^T W X) \beta + \frac{1}{2} \beta^T X^T W X \beta
\]

Note that this is a quadratic form, $\frac{1}{2}x^TAx + b^Tx + c$, with $A =
X^TWX$, $b=y^TWX$, and $c=0$.

We know that quadratic forms are minimized by $Ax - b = 0$, or $Ax = b$. Thus,
we conclude that $\hat{\beta}$ can be found by solving

\[
  (X^TWX)\hat{\beta} = y^TWX
\]

or equivalently, by

\[
  (X^TWX)\hat{\beta} = X^TWy
\]

\section{Solutions to the minimization}

The inversion method is a potentially awful way to solve the system. If the
entries of $X$ are all relatively similar to each other and the confidence
values differ wildly, the computed inverse may not be the inverse at all! More
generally, if the condition number of $X^TWX$ is large, then $X^{-1}X$ might not
be equal to the identity matrix!

%% Questions:
%% We could left-multiply by W^{-1}X^T^{-1} to get XB = y, but this seems to
%% throw away the weight information. Why can we not do this, or if we can,
%% why do the weights matter at all? A: Because X^T is probably not invertible dummy.
%% Is X^T W X guaranteed to be positive-definite? If so, we can use Cholesky.
%%

Since we cannot always use matrix inversion, we can instead use a matrix
factorization method. In this case, we choose an LU method because I'm not sure
if Choleky's conditions will hold here:

\begin{lstlisting}[mathescape = true]
  [L,U] = LU-factor($X^TWX$)
  z = $X^TWy$
  c = triangle-solve($Lc = z$)
  x = triangle-solve($Ux = b$)
  $\hat{\beta}$ = x
\end{lstlisting}

\section{Code}

Code for solving with inversion:

\lstinputlisting[language=Python]{inversionsolve.py}

Code for solving with LU factorization:

\lstinputlisting[language=Python]{factorsolve.py}

Unfortunately, due to the limitations of my current system (a celeron with 8GB
of memory), I was not able to do particularly large tests, since even relatively
small timing tests took up a significant amount of time.

The results for the three tests I did manage to finish are shown below. These
were done using iPython3's \texttt{\%timeit} magic. The elements of the matrices
are generated random uniform in $[0,50)$.

\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|c|c|}
    \hline
    Rows & Columns & Iterations & Factorization & Inversion \\ \hline \hline
    400 & 20 & 1000 & 2.44 & 2.54\\ \hline
    2000 & 40 & 100 & 18.2 & 19.7\\ \hline
    2000 & 200 & 100 & 33.3 & 36.3\\ \hline
  \end{tabular}
  \caption{Timings for factorization and inverse-based solutions in python. The
    times are for a single solve, averaged over the number of iterations (e.g.
    5s with 100 iterations means the run took 500 seconds total). All times are
    in seconds.}
  \label{tab: solvecomparetimings}
\end{table}

\section{Sparse Matrices}

Converting the code to use sparse matrix representation is pretty simple to do
with python. Simply use the sparse solver provided by
\texttt{scipy.sparse.linalg} to solve the problem. Stacking this solver up
against the previous solver, we get the following values:

\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    Rows & Columns & Sparsity & Sparse Solver (ms) & Inversion (ms) & Factorization (ms) \\ \hline \hline
    4000 & 200 & 0.05 & 1.63 & 1.15 & 0.58 \\ \hline
    4000 & 400 & 0.05 & 13.3 & 4.33 & 2.05 \\ \hline
    4000 & 800 & 0.05 & 65.1 & 23.4 & 10.9 \\ \hline
    4000 & 1600 & 0.05 & 383 & 151 & 52.4 \\ \hline
    4000 & 200 & 0.15 & 1.77 & 1.15 & 0.58\\ \hline
    4000 & 400 & 0.15 & 14   & 4.17 & 2.02\\ \hline
    4000 & 800 & 0.15 & 68.4 & 23.5 & 10.2\\ \hline
    4000 & 1600 & 0.15 & 394 & 157 & 50.9 \\ \hline
  \end{tabular}
  \caption{Timings for factorization and inverse-based, as well as sparse solver
    solutions in python. The non-sparse solvers were run by first converting the
  sparse matrix representation to a dense one, then using the routines provided
  by the previous test. All tests were done over 100 iterations. }
  \label{tab: solvecomparetimings}
\end{table}

\section{MLE}

The MLE form is:

\begin{align*}
  l(\beta) &= -\log\left\{ \prod\limits_{i=1}^N p(y_i \, ;\, \beta) \right\}\\
           &= - \sum\limits_{i=1}^N \log p(y_i \, ;\, \beta) \\
           &= -\sum\limits_{i=1}^N \log \left( \binom{m_i}{y_i} (w_i)^{y_i}(1 - w_i)^{m_i - y_i} \right)\\
           &= -\sum\limits_{i=1}^N \log \binom{m_i}{y_i} -\sum\limits_{i=1}^N \log w_i^{y_i} -\sum\limits_{i=1}^N \log (1 - w_i)^{m_i - y_i} \\
           &= -\sum\limits_{i=1}^N \log \binom{m_i}{y_i}
             -  \sum\limits_{i=1}^N \log 1 +  y_i \sum\limits_{i=1}^N \log (1 + \exp(x_i^T \beta)) -
             (m_i - y_i)\sum\limits_{i=1}^N \log\left(\frac{\exp(x_i^T\beta)}{1 + \exp(x_i^T \beta)}\right)
\end{align*}

The $\log 1$ term is identically equal to zero, and when we take the gradient,
we find that the binomial coefficient also goes to zero (since it does not
depend on $\beta$.) 

If we define $l_i(\beta)$ as 

\[
  l_i(\beta) =  y_i \sum\limits_{i=1}^N \log (1 + \exp(x_i^T \beta)) -
   (m_i - y_i) \log\left(\frac{\exp(x_i^T\beta)}{1 + \exp(x_i^T \beta)}\right) - \log \binom{m_i}{y_i}
\]

we can express $l(\beta) = \sum l_i(\beta)$. Then to find the gradient of $l$,
we need only find the gradient of each $l_i$.

\begin{align*}
 \nabla l_i(\beta) &=  \nabla \log (1 + \exp\{x_i^T \beta\}) -
                   \nabla \log\left(\frac{\exp(x_i^T\beta)}{1 + \exp(x_i^T \beta)}\right)\\
                   &= y_i x_i \left( \frac{\exp(x_i^T \beta)}{1 + \exp(x_i^T \beta)} \right)
                     - (m_i - y_i) x_i \left( \frac{\exp(x_i^T\beta)}{(\exp(x_i^T \beta) + 1)^2} \right)
\end{align*}

which, using the abbreviation mentioned in the assignment, is equivalent to

\[
  y_iw_i \exp(x_i^T \beta) x_i - (m_i - y_i) w_i^2 \exp(x_i^T \beta) x_i
\]

I have no idea if this is even close to correct.

\end{document}