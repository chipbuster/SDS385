{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lda\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordPool = lda.WordPool('testdocs')\n",
    "with open('lambda.pickle','rb') as pcklfile:\n",
    "    lambdas = pickle.load(pcklfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in range(4):\n",
    "    lambdas[k,:] = lambdas[k,:] / np.linalg.norm(lambdas[k,:],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "means = np.mean(lambdas,axis=0)\n",
    "notableList = []\n",
    "\n",
    "for j in range(4):\n",
    "    for k in range(np.shape(lambdas)[1]):\n",
    "        if lambdas[j,k] > 1.5 * means[k]:\n",
    "            notableList.append((j,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics = [[],[],[],[]]\n",
    "\n",
    "for (topicNum, wordID) in notableList:\n",
    "    topics[topicNum].append(wordPool.indexToWordTbl[wordID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ballerina', 'afterlife', 'kimono', 'boo', 'cadaver', 'vanish', 'robe', 'gruesome', 'fantasy', 'goblin', 'skeleton', 'weird', 'fairy', 'flashlight', 'coffin', 'bat', 'superhero', 'spook', 'princess', 'queen', 'strange', 'startling', 'phantasm', 'dreadful']\n",
      "['lever', 'square', 'lathe', 'hone', 'wrench', 'ladder', 'squeegee', 'toolmaker', 'chisel', 'adze', 'spade', 'saw', 'drill', 'spear', 'poker', 'edger', 'scalpel', 'tongs', 'scraper', 'machete', 'countersink', 'anvil', 'razor', 'clamp', 'fastener', 'trowel', 'wedge', 'shovel', 'bellows', 'stapler', 'ratchet', 'pinch', 'hammer', 'brush', 'sawhorse', 'cutters', 'carpenter', 'bolt', 'snips', 'scissors', 'sharpener', 'brad', 'crowbar', 'bevel', 'mallet', 'tiller', 'plow', 'rivet', 'pickaxe', 'tarragon', 'feed']\n",
      "['brownie', 'sole', 'durian', 'custard', 'chard', 'roast', 'toffee', 'ice', 'lobster', 'peach', 'dry', 'breadfruit', 'tortilla', 'cheddar', 'noodles', 'cheesecake', 'eggplant', 'banana', 'citron', 'recipe', 'macaroon', 'honeydew', 'liver', 'lollipop', 'cucumber', 'cauliflower', 'caramel', 'chow', 'ginger', 'beef', 'glasses', 'pomegranate', 'chicken', 'tapioca', 'sauce', 'cantaloupe', 'frosting', 'nectarine', 'ladle', 'taro', 'julienne', 'plate', 'spinach', 'onion', 'marionberry', 'boysenberry', 'order', 'beancurd', 'cream', 'savory', 'strudel', 'pitcher', 'punch', 'batter', 'whey', 'stew', 'lentils', 'kiwi', 'spoon', 'pomelo', 'grated', 'crust', 'steak', 'melon', 'raspberry', 'carrot', 'poached', 'mint', 'sweet', 'buns', 'cookbook', 'cobbler', 'salad', 'soy', 'patty', 'sherbet', 'caviar', 'celery', 'mango', 'lunch', 'mustard', 'blueberry', 'munch', 'seeds', 'ravioli', 'restaurant', 'popovers', 'teriyaki', 'entree', 'meatball', 'taco', 'sustenance', 'coleslaw', 'granola', 'meatloaf', 'lychee', 'cupcake', 'marshmallow', 'hummus', 'flax', 'marmalade', 'sausage', 'casserole', 'kitchen', 'strawberry', 'kohlrabi', 'rations', 'turkey', 'maize', 'teapot', 'straw', 'pork', 'fennel', 'cassava', 'crunch', 'boil', 'fruit', 'bran', 'cuisine', 'grain', 'olive', 'hazelnut', 'walnut', 'cake', 'spaghetti', 'kettle', 'tomato', 'soybeans', 'mochi', 'basil', 'nourish', 'peanut', 'gravy', 'lamb', 'juice', 'dine', 'persimmon', 'sandwich', 'grub', 'buckwheat', 'egg', 'micronutrient', 'milk', 'daikon', 'jelly', 'guava', 'oyster', 'hunger', 'minerals', 'gelatin', 'diet', 'lettuce', 'foodstuffs', 'hamburger', 'syrup', 'greenbean', 'drink', 'cater', 'popcorn', 'rhubarb', 'fire', 'mug', 'elderberry', 'pretzel', 'fritter', 'sage', 'peapod', 'pasta', 'sprouts', 'mozzarella', 'greens', 'brunch', 'tamale', 'crepe', 'chef', 'watermelon', 'clam', 'pilaf', 'cornflakes', 'broccoli', 'bacon', 'spareribs', 'yolk', 'romaine', 'feast', 'hayride', 'evil', 'shadowy', 'superstition', 'werewolf', 'genie', 'mist', 'king', 'pancake', 'chives', 'pumpernickel', 'rye', 'jalapeno', 'yam', 'cashew', 'barley', 'popsicle', 'cod', 'zucchini', 'chew', 'pretend', 'radish', 'pot', 'fig', 'berry', 'legumes', 'oleo', 'oats', 'cereal', 'roll', 'dairy', 'fry', 'nutrient', 'brisket', 'nosh', 'dip', 'tomatillo', 'cinnamon', 'mayonnaise', 'cloves', 'omelet', 'horseradish', 'currants', 'honey', 'garlic', 'fast', 'pie', 'dragonfruit', 'suet', 'lard', 'fat', 'kumquat', 'soysauce', 'tuber', 'saffron', 'papaya', 'wasabi', 'molasses', 'dressing', 'cranberry', 'bake', 'potato', 'menu', 'sushi', 'barbecue', 'tofu', 'refrigerator', 'tart', 'take-out', 'jug', 'jam', 'digest']\n",
      "['lox', 'stomach', 'biscuit', 'relish', 'burrito', 'venison', 'taro', 'spatula', 'cookbook', 'vegetable', 'cornmeal', 'lychee', 'waffle', 'cook', 'fillet', 'mints', 'nutrition', 'dish', 'dried', 'macaroni', 'wheat', 'ham', 'vitamin', 'okra', 'jellybeans', 'fathom', 'tide', 'lifeline', 'seafarer', 'foresail', 'abeam', 'listing', 'submersible', 'riverboat', 'anchor', 'capsize', 'figurehead', 'starboard', 'helm', 'barge', 'winch', 'current', 'funnel', 'runabout', 'junk', 'wheelhouse', 'navigate', 'steamboat', 'ship', 'helmsman', 'keel', 'propeller', 'plum', 'parsnip', 'pepper', 'freezer', 'utensils', 'shallots', 'capers', 'toast', 'citrus', 'platter', 'jug']\n"
     ]
    }
   ],
   "source": [
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doctopics = [1,1,1,1]\n",
    "doctopics[0] = open('wordlist1.txt','r').read().split()\n",
    "doctopics[1] = open('wordlist2.txt','r').read().split()\n",
    "doctopics[2] = open('wordlist3.txt','r').read().split()\n",
    "doctopics[3] = open('wordlist4.txt','r').read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percents = np.zeros((4,4))\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        numInBoth = len(set(topics[i]).intersection(set(doctopics[j])))\n",
    "        numInLDA = len(topics[i])\n",
    "        percents[i,j] = numInBoth / numInLDA\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          1.          0.        ]\n",
      " [ 0.03921569  0.96078431  0.          0.01960784]\n",
      " [ 0.96498054  0.          0.03501946  0.        ]\n",
      " [ 0.57142857  0.          0.          0.42857143]]\n"
     ]
    }
   ],
   "source": [
    "print(percents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
